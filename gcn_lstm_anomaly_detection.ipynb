{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VSIE43\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.csv', '1.csv', '10.csv', '11.csv', '12.csv', '13.csv', '14.csv', '15.csv', '2.csv', '3.csv', '4.csv', '5.csv', '6.csv', '7.csv', '8.csv', '9.csv']\n",
      "['0.csv', '1.csv', '2.csv', '3.csv']\n"
     ]
    }
   ],
   "source": [
    "data_pth = r'C:\\Users\\VSIE43\\OneDrive - Scania CV\\Thesis\\SKAB-master\\data/'\n",
    "normal_file = 'anomaly-free/anomaly-free.csv'\n",
    "test_1 = 'other'\n",
    "test_2 = 'valve1'\n",
    "test_3 = 'valve2'\n",
    "\n",
    "df = pd.read_csv(data_pth+normal_file, sep=';')\n",
    "\n",
    "df.columns\n",
    "\n",
    "def fillin_time_gaps(df):\n",
    "    df.datetime = pd.to_datetime(df.datetime.values)\n",
    "    time_diff = np.diff(df.datetime.values)\n",
    "\n",
    "    # there will be need for data imputation. \n",
    "    # some samples are with differnce of 2 seconds, rather than 1 second\n",
    "    new_time = pd.date_range(df.datetime.min(), df.datetime.max(),freq='1s')\n",
    "    missing_time = pd.DataFrame({'datetime' : new_time})\n",
    "    df_new = missing_time.merge(df, on='datetime', how='left')\n",
    "\n",
    "    # maybe fill in with interpolation\n",
    "    df_new = df_new.interpolate(method='ffill')\n",
    "    return df_new\n",
    "\n",
    "normal_data = []\n",
    "df = pd.read_csv(data_pth+normal_file, sep=';')\n",
    "normal_data.append(fillin_time_gaps(df).drop(columns=['datetime']))\n",
    "for folder in [test_2, test_3]:\n",
    "    files = os.listdir(data_pth+folder)\n",
    "    print(files)\n",
    "    for file in files:\n",
    "        tmp = pd.read_csv(data_pth+folder+'/'+file, sep=';')\n",
    "        # print(tmp.columns)\n",
    "        # t = tmp[tmp.anomaly==1]\n",
    "        t=tmp[tmp.anomaly==0]\n",
    "        t = t.drop(columns=['changepoint'])\n",
    "        normal_data.append(t)\n",
    "\n",
    "df = fillin_time_gaps(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph from NTS-No Tears\n",
    "adj = np.array([[1]+[0]*6+[1], [0]*2+[1]+[0]*5, [0]*2+[1]+[0]*5, [1]+[0]*2+[1]*2+[0]+[1]*2, [0]*4+[1]+[0]*3, [0]*8, [1]+[0]*5+[1]+[0], [0]*6+[1]*2 ])\n",
    "def adj_to_edge_index(adj):\n",
    "    edge_index = []\n",
    "    num_nodes = adj.shape[0]\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if adj[i, j] == 1:\n",
    "                edge_index.append([i, j])\n",
    "    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "edge_index = adj_to_edge_index(adj)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporal_windows_train shape: (9945, 16, 8, 1)\n",
      "temporal_windows_train_next_step shape: (9945, 16, 8, 1)\n",
      "temporal_windows_test shape: (1139, 16, 8, 1)\n",
      "temporal_windows_test_next_step shape: (1139, 16, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "def min_max_normalize(df, m_m_params=None):\n",
    "    \n",
    "    if m_m_params:\n",
    "        (min_p, max_p) = m_m_params\n",
    "    else:\n",
    "        (min_p, max_p) = df.min(), df.max()\n",
    "\n",
    "    new_df = (df-min_p) / (max_p-min_p)\n",
    "\n",
    "    return new_df,  (min_p, max_p)\n",
    "\n",
    "\n",
    "data, m_m_params = min_max_normalize(df.drop(columns=['datetime']))\n",
    "\n",
    "test_df_1 = pd.read_csv(data_pth+test_1+'/5.csv',sep=';')\n",
    "test_df_1_norm, _  = min_max_normalize(test_df_1.drop(columns=['datetime','anomaly','changepoint']), m_m_params)\n",
    "\n",
    "# Enable synchronous error reporting\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Function to create temporal node features\n",
    "def create_temporal_features(df):\n",
    "    features = []\n",
    "    for t in range(len(df)):\n",
    "        x_t = torch.tensor(df.iloc[t].values, dtype=torch.float).view(-1, 1)\n",
    "        features.append(x_t)\n",
    "    return features\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# train_data = data[:int(0.8*len(data))]\n",
    "# val_data = data[int(0.8*len(data)):]\n",
    "\n",
    "# # Normalize the validation data using the same min-max parameters\n",
    "# val_data, _ = min_max_normalize(val_data, m_m_params)\n",
    "\n",
    "# Create temporal node features for training, validation, and testing\n",
    "temporal_features_train = create_temporal_features(data)\n",
    "# temporal_features_val = create_temporal_features(val_data)\n",
    "temporal_features_test = create_temporal_features(test_df_1_norm)\n",
    "\n",
    "def create_sliding_windows(features, window_size, stride):\n",
    "    windows = []\n",
    "    next_step_t = []\n",
    "\n",
    "    for i in range(0, len(features) - window_size, stride):\n",
    "        window = features[i:i + window_size]\n",
    "        next_step_window = features[i + 1:i + window_size + 1]\n",
    "        \n",
    "        if len(next_step_window) == window_size:  # Ensure that the next step window has the same length as the window\n",
    "            windows.append(window)\n",
    "            next_step_t.append(next_step_window)\n",
    "\n",
    "    return np.array(windows), np.array(next_step_t)\n",
    "\n",
    "# Example usage\n",
    "window_size = 16  # Define your window size\n",
    "stride = 1  # Define the stride for the sliding window\n",
    "\n",
    "# Create sliding windows for training, validation, and testing\n",
    "temporal_windows_train, temporal_windows_train_next_step = create_sliding_windows(temporal_features_train, window_size, stride)\n",
    "# temporal_windows_val, temporal_windows_val_next_step = create_sliding_windows(temporal_features_val, window_size, stride)\n",
    "temporal_windows_test, temporal_windows_test_next_step = create_sliding_windows(temporal_features_test, window_size, stride)\n",
    "\n",
    "print(f\"temporal_windows_train shape: {temporal_windows_train.shape}\")\n",
    "print(f\"temporal_windows_train_next_step shape: {temporal_windows_train_next_step.shape}\")\n",
    "print(f\"temporal_windows_test shape: {temporal_windows_test.shape}\")\n",
    "print(f\"temporal_windows_test_next_step shape: {temporal_windows_test_next_step.shape}\")\n",
    "\n",
    "assert len(temporal_windows_train) == len(temporal_windows_train_next_step), \"Mismatch in training windows and next steps\"\n",
    "assert len(temporal_windows_test) == len(temporal_windows_test_next_step), \"Mismatch in testing windows and next steps\"\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, windows, next_step_t):\n",
    "        self.windows = windows\n",
    "        self.next_step_t = next_step_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        next_step_window = self.next_step_t[idx]\n",
    "        return window, next_step_window\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move edge_index to the GPU\n",
    "num_nodes = len(data.columns)\n",
    "\n",
    "# Fully Connected Graph with Self Connections\n",
    "# edge_index = torch.tensor([(i, j) for i in range(num_nodes) for j in range(num_nodes) if i != j], dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "# Fully Connected Graph without self connections\n",
    "edge_index = torch.tensor([(i, j) for i in range(num_nodes) for j in range(num_nodes) if i != j], dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = TimeSeriesDataset(temporal_windows_train, temporal_windows_train_next_step)\n",
    "# val_dataset = TimeSeriesDataset(temporal_windows_val, temporal_windows_val_next_step)\n",
    "test_dataset = TimeSeriesDataset(temporal_windows_test, temporal_windows_test_next_step)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_skab = np.savez('train_skab', data.to_numpy())\n",
    "test_skab = np.savez('test_skab', test_df_1_norm.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_nodes):\n",
    "        super(GCN_LSTM, self).__init__()\n",
    "        self.gcn1 = GCNConv(in_channels, hidden_channels * 2)\n",
    "        self.gcn2 = GCNConv(hidden_channels * 2, hidden_channels * 2)  # Second GCN layer\n",
    "        self.gcn3 = GCNConv(hidden_channels * 2, hidden_channels * 2)\n",
    "        self.lstm1 = nn.LSTM(hidden_channels * 2, hidden_channels * 2, num_layers=3, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_channels * 2, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        batch_size, sequence_length, num_nodes = x.size()\n",
    "        gcn_outputs = []\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            gcn_output = self.gcn1(x[:, t, :].view(-1, num_nodes), edge_index)\n",
    "            gcn_output = F.relu(gcn_output)\n",
    "            gcn_output = self.gcn2(gcn_output, edge_index)  # Pass through the second GCN layer\n",
    "            gcn_output = F.relu(gcn_output)\n",
    "            gcn_output = self.gcn3(gcn_output, edge_index)\n",
    "            gcn_output = F.relu(gcn_output)\n",
    "            gcn_output = gcn_output.view(batch_size, num_nodes, -1)\n",
    "            gcn_outputs.append(gcn_output)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs, dim=1)  # Shape: [batch_size, sequence_length, num_nodes, hidden_channels*2]\n",
    "        gcn_outputs = gcn_outputs.view(batch_size, sequence_length, -1)  # Flatten the node dimension for LSTM\n",
    "\n",
    "        lstm_output, _ = self.lstm1(gcn_outputs)  # Shape: [batch_size, sequence_length, hidden_channels*2]\n",
    "\n",
    "        next_step = self.decoder(lstm_output)  # Shape: [batch_size, sequence_length, out_channels]\n",
    "\n",
    "        return next_step\n",
    "    \n",
    "# Initialize the model, optimizer, and other components\n",
    "in_channels = 8  # One feature per node (the time-series value at each timestep)\n",
    "hidden_channels = 16\n",
    "out_channels = 8\n",
    "num_nodes = len(data.columns)\n",
    "\n",
    "# model = GCN_LSTM(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels, num_nodes=num_nodes).to(device)\n",
    "model = GCN_LSTM(in_channels=in_channels, hidden_channels=hidden_channels, out_channels=out_channels, num_nodes=num_nodes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, amsgrad=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, num_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    all_train_predictions = []\n",
    "    all_train_targets = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n",
    "            windows, next_step_windows = batch\n",
    "            # print(windows.size())\n",
    "            # Move data to the appropriate device (CPU or GPU)\n",
    "            # windows = windows.to(device)\n",
    "            # windows = windows\n",
    "            # next_step_windows = next_step_windows.to(device)\n",
    "            # next_step_windows = next_step_windows\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Remove the extra dimension\n",
    "            windows = windows.squeeze(-1)\n",
    "            next_step_windows = next_step_windows.squeeze(-1) \n",
    "\n",
    "            # Assuming edge_index is available and already moved to the device\n",
    "            next_step_pred = model(windows, edge_index)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(next_step_pred, next_step_windows)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and targets for analysis\n",
    "            all_train_predictions.append(next_step_pred.detach().cpu().numpy())\n",
    "            all_train_targets.append(next_step_windows.detach().cpu().numpy())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # # Validation\n",
    "        # model.eval()\n",
    "        # val_loss = 0\n",
    "        # with torch.no_grad():\n",
    "        #     for val_batch in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n",
    "        #         windows, next_step_windows = val_batch\n",
    "\n",
    "        #         # Move data to the appropriate device (CPU or GPU)\n",
    "        #         windows = windows.to(device)\n",
    "        #         next_step_windows = next_step_windows.to(device)\n",
    "\n",
    "        #         # Remove the extra dimension\n",
    "        #         windows = windows.squeeze(-1)\n",
    "        #         next_step_windows = next_step_windows.squeeze(-1)  # Use the last timestep for the target\n",
    "\n",
    "        #         # Assuming edge_index is available and already moved to the device\n",
    "        #         next_step_pred = model(windows, edge_index)\n",
    "\n",
    "        #         # Compute loss\n",
    "        #         loss = F.mse_loss(next_step_pred, next_step_windows)\n",
    "        #         val_loss += loss.item()\n",
    "\n",
    "        # avg_val_loss = val_loss / len(val_loader)\n",
    "        # val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, all_train_predictions, all_train_targets\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 40\n",
    "train_losses, val_losses, train_predictions, train_targets = train(train_loader, model, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test_loader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Inference Batches\", leave=False):\n",
    "            windows, next_step_windows = batch\n",
    "\n",
    "\n",
    "            # Move data to the appropriate device (CPU or GPU)\n",
    "            # windows = windows.to(device)\n",
    "            # next_step_windows = next_step_windows.to(device)\n",
    "\n",
    "            # Remove the extra dimension\n",
    "            windows = windows.squeeze(-1)\n",
    "            next_step_windows = next_step_windows.squeeze(-1)\n",
    "\n",
    "\n",
    "            # Assuming edge_index is available and already moved to the device\n",
    "            next_step_pred = model(windows, edge_index)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(next_step_pred, next_step_windows)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and targets for analysis\n",
    "            all_predictions.append(next_step_pred.cpu().numpy())\n",
    "            all_targets.append(next_step_windows.cpu().numpy())\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    return average_loss, all_predictions, all_targets\n",
    "\n",
    "# Perform inference on the test data\n",
    "test_loss, test_predictions, test_targets = inference(test_loader, model)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = []\n",
    "predicted_values = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in train_loader:\n",
    "        # sequences = sequences.squeeze(-1).to(device)\n",
    "        sequences = sequences.squeeze(-1)\n",
    "        # labels = labels.squeeze(-1).to(device)\n",
    "        labels = labels.squeeze(-1)\n",
    "        outputs = model(sequences, edge_index)\n",
    "        actual_values.extend(labels.cpu().numpy())\n",
    "        predicted_values.extend(outputs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_train = np.sum((np.array(actual_values)-np.array(predicted_values)), axis=(1))\n",
    "plt.hist(residual_train)\n",
    "threshold = np.percentile(residual_train, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "err_median = np.percentile(residual_train, 50,  axis=(0))\n",
    "err_iqr = iqr(residual_train,axis=(0))\n",
    "\n",
    "epsilon=1e-2\n",
    "print((residual_train).shape, err_median.shape, err_iqr.shape)\n",
    "residual_tmp_train = np.max((residual_train - err_median) / ( np.abs(err_iqr) +epsilon), axis=(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(residual_tmp_train, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_test = np.sqrt(np.sum((test_predictions-test_targets)**2, axis=(1,2)))\n",
    "\n",
    "plt.plot(residual_test)\n",
    "plt.plot(test_df_1.anomaly)\n",
    "# plt.axhline(y = threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_df_1.anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions and targets for further analysis\n",
    "np.save('test_predictions.npy', test_predictions)\n",
    "np.save('test_targets.npy', test_targets)\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the losses\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "preds = []\n",
    "labels_tmp = []\n",
    "file_number = []\n",
    "# m = model_ext\n",
    "residual_tmp_all = []\n",
    "\n",
    "thresholds = [1 , 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50]\n",
    "\n",
    "def f1_score(true, pred):\n",
    "    return metrics.f1_score(true, pred)\n",
    "\n",
    "def recall_score(true, pred):\n",
    "    return metrics.recall_score(true, pred)\n",
    "\n",
    "def auc_score(true, pred):\n",
    "    return metrics.roc_auc_score(true, pred)\n",
    "\n",
    "def fpr_score(true, pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(true, pred).ravel()\n",
    "    return fp / (fp + tn)\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results = {\n",
    "    'f1_score': [],\n",
    "    'fpr_score': [],\n",
    "    'recall_score': [],\n",
    "    'auc_score': []\n",
    "}\n",
    "\n",
    "results = {th: {'preds': []} for th in thresholds}\n",
    "\n",
    "files = os.listdir(data_pth+test_1)\n",
    "model.eval()\n",
    "print(files)\n",
    "for file in files:\n",
    "    print(file)\n",
    "    tmp_actual = []\n",
    "    tmp_pred = []\n",
    "    tmp = pd.read_csv(data_pth+test_1+'/'+file, sep=';')\n",
    "    \n",
    "    t = tmp.anomaly.values\n",
    "    test_df_1_tmp, _  = min_max_normalize(tmp.drop(columns=['datetime','anomaly','changepoint']),m_m_params)\n",
    "    temporal_windows_tmp, temporal_windows_tmp_next_step = create_sliding_windows(test_df_1_tmp, window_size = 16, stride = 1)\n",
    "    # print(temporal_windows_tmp.shape)\n",
    "    # print(temporal_windows_tmp_next_step.shape)\n",
    "    tmp_dataset = TimeSeriesDataset(temporal_windows_tmp, temporal_windows_tmp_next_step)\n",
    "    # Create DataLoader objects\n",
    "    tmp_loader = DataLoader(tmp_dataset, batch_size=64, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in tmp_loader:\n",
    "            sequences = sequences.squeeze(-1).float()\n",
    "            print(sequences.shape)\n",
    "            labels = labels.squeeze(-1).float()\n",
    "            outputs = model(sequences, edge_index)\n",
    "            tmp_actual.extend(labels.cpu().numpy())\n",
    "            tmp_pred.extend(outputs.cpu().numpy())\n",
    "    residual_tmp = np.sqrt(np.sum((np.array(tmp_pred)-np.array(tmp_actual))**2, axis=(1,2)))\n",
    "\n",
    "    residual_tmp_all.append(residual_tmp)\n",
    "    labels_tmp.append(t[-len(tmp_pred):])\n",
    "    i = int(file.split('.')[0])\n",
    "    file_number.append([i]*len(tmp_pred))\n",
    "    # for th in thresholds:\n",
    "    #     results[th]['preds'].append(residual_tmp > th)\n",
    "    i +=1\n",
    "    # break\n",
    "# for th in thresholds:\n",
    "#     results[th]['preds'] = np.concatenate(results[th]['preds'], axis=0)\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "labels = np.concatenate(labels_tmp, axis=0)\n",
    "file_number = np.concatenate(file_number, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_anom_list = []\n",
    "for file in files:\n",
    "    tmp_df = pd.read_csv(data_pth+test_1+'/'+file, sep=';')\n",
    "    tmp_anom_list.append(tmp_df.anomaly)\n",
    "    plt.plot(tmp_df.anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp_anom_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residual_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_tmp_all = np.concatenate(residual_tmp_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "for i in range(len(residual_tmp_all)):\n",
    "    err_median = np.median(residual_tmp_all[i])\n",
    "    err_iqr = iqr(residual_tmp_all[i])\n",
    "\n",
    "    epsilon=1e-2\n",
    "\n",
    "    residual_tmp_all[i] = (residual_tmp_all[i] - err_median) / ( np.abs(err_iqr) +epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(residual_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(residual_tmp_all)):\n",
    "    fig, ax1 = plt.subplots()  # Create a new figure and axis for residuals\n",
    "\n",
    "    ax1.plot(residual_tmp_all[i], 'b-')\n",
    "    ax1.set_title(f'Residual for File {i+1}')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Residuals', color='b')\n",
    "\n",
    "    ax2 = ax1.twinx()  # Create a second y-axis\n",
    "    ax2.plot(tmp_anom_list[i], 'r-')\n",
    "    ax2.set_ylabel('Anomaly', color='r')\n",
    "    ax2.set_ylim(-0.1, 1.1)  # Set limits for the anomaly axis\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Define evaluation metrics functions\n",
    "def f1_score(true, pred):\n",
    "    return metrics.f1_score(true, pred)\n",
    "\n",
    "def recall_score(true, pred):\n",
    "    return metrics.recall_score(true, pred)\n",
    "\n",
    "def auc_score(true, pred):\n",
    "    return metrics.roc_auc_score(true, pred)\n",
    "\n",
    "def fpr_score(true, pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(true, pred).ravel()\n",
    "    return fp / (fp + tn)\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results = {\n",
    "    'f1_score': [],\n",
    "    'fpr_score': [],\n",
    "    'recall_score': [],\n",
    "    'auc_score': []\n",
    "}\n",
    "\n",
    "# Loop through the files and calculate the metrics\n",
    "for f in range(1, 15):\n",
    "    true = labels[file_number == f]\n",
    "    pred = preds.astype(int)[file_number == f]\n",
    "    \n",
    "    results['f1_score'].append(f1_score(true, pred))\n",
    "    results['fpr_score'].append(fpr_score(true, pred))\n",
    "    results['recall_score'].append(recall_score(true, pred))\n",
    "    results['auc_score'].append(auc_score(true, pred))\n",
    "print(results)\n",
    "# Create a DataFrame from the results dictionary\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Define the file path to save the results\n",
    "file_path = 'graphvs_ae_anomaly_detection_scores.xlsx'\n",
    "\n",
    "# Save the DataFrame to a new sheet in the existing Excel file\n",
    "with pd.ExcelWriter(file_path, mode='a', engine='openpyxl') as writer:\n",
    "    df.to_excel(writer, sheet_name='Additional Scores', index=False)\n",
    "\n",
    "print(\"Results have been saved to a new sheet in\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_results = {th: {'f1_score': [], 'fpr_score': [], 'recall_score': [], 'auc_score': []} for th in thresholds}\n",
    "\n",
    "# Loop through the thresholds to calculate the metrics for each file\n",
    "for th in thresholds:\n",
    "    # The preds are now directly accessed from the 'results' dictionary\n",
    "    for f in range(1, 15):  # Adjust range as per your actual file indices\n",
    "        true = labels[file_number == f]\n",
    "        pred = results[th]['preds'].astype(int)[file_number == f]\n",
    "        # print(len(pred))\n",
    "        metrics_results[th]['f1_score'].append(f1_score(true, pred))\n",
    "        metrics_results[th]['fpr_score'].append(fpr_score(true, pred))\n",
    "        metrics_results[th]['recall_score'].append(recall_score(true, pred))\n",
    "        metrics_results[th]['auc_score'].append(auc_score(true, pred))\n",
    "\n",
    "# If you need to print or analyze aggregated results:\n",
    "for th in thresholds:\n",
    "    print(f\"Threshold {th}:\")\n",
    "    print(f\"F1 Score: {np.mean(metrics_results[th]['f1_score'])}\")\n",
    "    print(f\"FPR Score: {np.mean(metrics_results[th]['fpr_score'])}\")\n",
    "    print(f\"Recall Score: {np.mean(metrics_results[th]['recall_score'])}\")\n",
    "    print(f\"AUC Score: {np.mean(metrics_results[th]['auc_score'])}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "data_frames = []\n",
    "for th, scores in metrics_results.items():\n",
    "    # Calculate the mean of each list of scores for the current threshold\n",
    "    mean_scores = {metric: np.mean(values) for metric, values in scores.items()}\n",
    "    mean_scores['Threshold'] = th\n",
    "    # Convert mean_scores to a DataFrame and append to data_frames list\n",
    "    df = pd.DataFrame([mean_scores])  # Create a DataFrame with a single row of mean_scores\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all data frames into a single frame\n",
    "final_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# File path to your Excel file\n",
    "file_path = 'graphvs_ae_anomaly_detection_scores.xlsx'\n",
    "\n",
    "# Load the existing Excel file\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "    # Check if the file already exists; if not, write a new file\n",
    "    try:\n",
    "        _ = pd.read_excel(file_path)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, create it and write the DataFrame\n",
    "        final_df.to_excel(writer, index=False, sheet_name='Anomaly Detection Scores')\n",
    "    else:\n",
    "        # Write to a new sheet in the existing file\n",
    "        final_df.to_excel(writer, index=False, sheet_name='Anomaly Detection Scores')\n",
    "\n",
    "print(\"Data has been written to the Excel file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_number"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
